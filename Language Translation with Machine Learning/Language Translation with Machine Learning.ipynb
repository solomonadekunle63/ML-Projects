{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEpM958KeZhA"
   },
   "source": [
    "# Language Translation with Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o5MFtNEk2INf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import os\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yI_g1Bl3_1Ok",
    "outputId": "d91af095-e64f-4242-a46d-7b8820e549cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source                                   english_sentence  \\\n",
      "0        ted  politicians do not have permission to do what ...   \n",
      "1        ted         I'd like to tell you about one such child,   \n",
      "2  indic2012  This percentage is even greater than the perce...   \n",
      "3        ted  what we really mean is that they're bad at not...   \n",
      "4  indic2012  .The ending portion of these Vedas is called U...   \n",
      "\n",
      "                                      hindi_sentence  \n",
      "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
      "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
      "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
      "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
      "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "lines = pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\", encoding='utf-8')\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(lines.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvn--xUC_46R",
    "outputId": "f3284b92-598f-4d40-ee3c-97a5be4c6067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the dataset to include only rows where the source is 'ted'\n",
    "lines = lines[lines['source'] == 'ted']\n",
    "\n",
    "# Remove rows with null English sentences\n",
    "lines = lines[~pd.isnull(lines['english_sentence'])]\n",
    "\n",
    "# Drop duplicate rows\n",
    "lines.drop_duplicates(inplace=True)\n",
    "\n",
    "# Randomly sample 25,000 rows from the dataset\n",
    "lines = lines.sample(n=25000, random_state=42)\n",
    "\n",
    "# Check the shape of the resulting dataset\n",
    "lines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f85DWOt-AOOv"
   },
   "outputs": [],
   "source": [
    "# Convert English sentences to lowercase\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.lower())\n",
    "\n",
    "# Convert Hindi sentences to lowercase\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H7Xi9KAGAScn"
   },
   "outputs": [],
   "source": [
    "# Remove single quotes from English and Hindi sentences\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a9XSaQF3AWcl"
   },
   "outputs": [],
   "source": [
    "# Set of all special characters\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Remove all the special characters from English and Hindi sentences\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qOExERLUAa-E"
   },
   "outputs": [],
   "source": [
    "# Remove digits from English and Hindi sentences\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
    "\n",
    "# Remove specific Hindi characters\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
    "\n",
    "# Remove extra spaces and add special tokens\n",
    "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x.strip()))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x.strip()))\n",
    "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: 'START_ ' + x + ' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1xLZyp_sAe-U"
   },
   "outputs": [],
   "source": [
    "# Get English and Hindi vocabularies\n",
    "all_eng_words = set(word for eng in lines['english_sentence'] for word in eng.split())\n",
    "all_hindi_words = set(word for hin in lines['hindi_sentence'] for word in hin.split())\n",
    "\n",
    "# Calculate the length of each English and Hindi sentence\n",
    "lines['length_eng_sentence'] = lines['english_sentence'].apply(lambda x: len(x.split()))\n",
    "lines['length_hin_sentence'] = lines['hindi_sentence'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rXVZMSgLAivE"
   },
   "outputs": [],
   "source": [
    "# Filter out sentences longer than 20 words\n",
    "lines = lines[lines['length_eng_sentence'] <= 20]\n",
    "lines = lines[lines['length_hin_sentence'] <= 20]\n",
    "\n",
    "# Get the maximum length of source and target sentences\n",
    "max_length_src = max(lines['length_hin_sentence'])\n",
    "max_length_tar = max(lines['length_eng_sentence'])\n",
    "\n",
    "# Get sorted lists of unique words for both English and Hindi\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_hindi_words))\n",
    "\n",
    "# Get the number of tokens for both encoder and decoder\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_hindi_words)\n",
    "\n",
    "# Increment decoder tokens for zero padding\n",
    "num_decoder_tokens += 1  \n",
    "\n",
    "# Create token indexes for both languages\n",
    "input_token_index = dict([(word, i + 1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i + 1) for i, word in enumerate(target_words)])\n",
    "\n",
    "# Create reverse indexes for tokenization\n",
    "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
    "\n",
    "# Shuffle the dataset\n",
    "lines = shuffle(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PG-FqUaAnBn"
   },
   "source": [
    "## Training Model to Translate English to Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AwxnLjb0Anfg"
   },
   "outputs": [],
   "source": [
    "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training and testing sets as pickle files\n",
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbW1-cKQAsHP",
    "outputId": "3fb11119-7c6c-434b-8e2a-2895c1822c9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, None, 300)            4209000   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 300)            5262300   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 300),                721200    ['embedding[0][0]']           \n",
      "                              (None, 300),                                                        \n",
      "                              (None, 300)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, None, 300),          721200    ['embedding_1[0][0]',         \n",
      "                              (None, 300),                           'lstm[0][1]',                \n",
      "                              (None, 300)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 17541)          5279841   ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 16193541 (61.77 MB)\n",
      "Trainable params: 16193541 (61.77 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(X, y, batch_size):\n",
    "    '''\n",
    "    Generate a batch of data\n",
    "    Args:\n",
    "    - X: Input data\n",
    "    - y: Target data\n",
    "    - batch_size: Size of the batch\n",
    "    '''\n",
    "    while True:\n",
    "        for j in range(0, len(X), batch_size):\n",
    "            encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')\n",
    "            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')\n",
    "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
    "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
    "                for t, word in enumerate(input_text.split()):\n",
    "                    encoder_input_data[i, t] = input_token_index[word]  # Encoder input seq\n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    if t < len(target_text.split()) - 1:\n",
    "                        decoder_input_data[i, t] = target_token_index[word]  # Decoder input seq\n",
    "                    if t > 0:\n",
    "                        # Decoder target sequence (one hot encoded)\n",
    "                        # Does not include the START_ token\n",
    "                        # Offset by one timestep\n",
    "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
    "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
    "\n",
    "# Define model parameters\n",
    "latent_dim = 300\n",
    "batch_size = 128\n",
    "\n",
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]  # Keep the states\n",
    "\n",
    "# Define decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# Train parameters\n",
    "train_samples = len(X_train)\n",
    "val_samples = len(X_test)\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3MgVmSqYBqy8",
    "outputId": "d50f7bc8-e0ef-44e2-91e7-dbcb665fe4f0"
   },
   "outputs": [],
   "source": [
    "# Train the model using the generator for both training and validation data\n",
    "model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
    "                    validation_steps=val_samples//batch_size)\n",
    "\n",
    "# Save the trained model weights\n",
    "model.save_weights('nmt_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_26664\\995539701.py:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  3/154 [..............................] - ETA: 4:30 - loss: 9.7518InvalidArgumentError: Graph execution error:\n",
      "\n",
      "Detected at node model/embedding/embedding_lookup defined at (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_26664\\1230367736.py\", line 2, in <module>\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 2913, in fit_generator\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1807, in fit\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1150, in train_step\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 590, in __call__\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 515, in call\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\functional.py\", line 672, in _run_internal_graph\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py\", line 1149, in __call__\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 96, in error_handler\n",
      "\n",
      "  File \"C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py\", line 272, in call\n",
      "\n",
      "indices[11,0] = 14030 is not in [0, 14030)\n",
      "\t [[{{node model/embedding/embedding_lookup}}]] [Op:__inference_train_function_13317]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for and handle GPU memory growth\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Enable GPU memory growth to prevent allocation errors\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Train the model with error handling\n",
    "try:\n",
    "    model.fit_generator(generator=generate_batch(X_train, y_train, batch_size=batch_size),\n",
    "                        steps_per_epoch=train_samples//batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=generate_batch(X_test, y_test, batch_size=batch_size),\n",
    "                        validation_steps=val_samples//batch_size)\n",
    "    # Save the trained model weights\n",
    "    model.save_weights('nmt_weights.h5')\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    # Handle the InvalidArgumentError\n",
    "    print(\"InvalidArgumentError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "L--vbS09eErg"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the \"thought vectors\"\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)  \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character\n",
    "        if (sampled_char == '_END' or len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "# Initialize training data generator\n",
    "train_gen = generate_batch(X_train, y_train, batch_size=1)\n",
    "\n",
    "k = -1  # Assuming this is used as a counter/index variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1LAnQAh2eNUX",
    "outputId": "1d12ead1-dd7d-4f47-ab0e-da206717aad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "Input English sentence: another aha moment\n",
      "Actual Hindi Translation:  एक और अहा क्षण \n",
      "Predicted Hindi Translation:  \n"
     ]
    }
   ],
   "source": [
    "k += 1  # Increment k\n",
    "(input_seq, actual_output), _ = next(train_gen)  # Generate a batch of data\n",
    "decoded_sentence = decode_sequence(input_seq)  # Decode the input sequence\n",
    "\n",
    "# Print the input English sentence, actual Hindi translation, and predicted Hindi translation\n",
    "print('Input English sentence:', X_train[k:k+1].values[0])\n",
    "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])  # Exclude 'START_' and '_END' tokens\n",
    "print('Predicted Hindi Translation:', decoded_sentence[:-4])  # Exclude '_END' token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
